{"cells":[{"cell_type":"markdown","metadata":{},"source":["## GMM Estimation of Logit Model\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Consider a &ldquo;logit&rdquo; regression; score function from MLE is:\n$$\n     \\frac{1}{N}\\sum_j X_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n  $$\nLet $u_j = y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}$; if we have some $Z$ such that\n$\\mbox{E}(u|Z) = 0$ then we can construct further moment conditions\n$$\n     \\frac{1}{N}\\sum_j Z_j\\left(y_j - \\frac{e^{X_j\\beta}}{1+e^{X_j\\beta}}\\right) = 0.\n  $$\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### GMM Estimator\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n\n# Some different options for moments...\n\ndef mle(b,y,X,Z):\n    \"\"\"Observations of score for MLE estimator.\n\n    Moment condition is E(y_j - p(x_jb))x_j = 0,\n    where p(xb) = e^{xb}/(1+e^{xb})\n    \"\"\"\n    p = np.exp(X*b)  # This is actually the odds\n    p = p/(1+p)      # This is probability y=1\n\n    return X*(y - p)\n\ndef nonlinear_iv(b,y,X,Z):\n    \"\"\"Observations for restriction that Z\n    orthogonal to score.\n\n    Moment condition is E(Z_jy_j - Z_jexp(x_jb)) = 0\n    \"\"\"\n    p = np.exp(X*b)  # This is actually the odds\n    p = p/(1+p)      # This is probability y=1\n\n    return Z*(y - p)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generating Process\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from scipy.stats import distributions as iid\n\ndef dgp(N,beta,VXZ,gamma=1):\n    \"\"\"Generate a tuple of (y,X,Z).\n\n    Satisfies model:\n        Pr(y=1|X) = f(X@beta,gamma)\n        u = y - f(X@beta,gamma)\n        E(u|Z) = 0\n        f(x,gamma) = (exp(x)/(1+exp(x)))**gamma\n        Var([X,Z}) = VXZ\n        X,Z mean zero, Gaussian\n\n    Each element of the tuple is an array of N observations.\n    When gamma=1 this reduces to the logit model\n\n    Inputs include\n    - beta :: Governs effect of X on probability y=1\n    - gamma :: Governs curvature of function\n    - VXZ :: Var([X,Z])\n    \"\"\"\n    \n    # \"Square root\" of VXZ via eigendecomposition\n    lbda,v = np.linalg.eig(VXZ)\n    SXZ = v@np.diag(np.sqrt(lbda))\n\n    # Generate normal random variates [X*,Z]\n    XZ = iid.norm.rvs(size=(N,VXZ.shape[0]))@SXZ.T\n\n    X = XZ[:,[0]] \n    Z = XZ[:,1:]\n\n    # Calculate y\n    pi = np.exp(X*beta)\n    pi = (pi/(1+pi))**gamma\n\n    y = iid.bernoulli(pi).rvs(size=(N,1))\n\n    return y,X,Z"]},{"cell_type":"markdown","metadata":{},"source":["### The Truth (Mark I)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this version of the truth the form of the distribution of $y$ is\nknown up to the parameter $\\beta$; MLE takes advantage of this.\n\nChoose some parameters to establish the &ldquo;truth&rdquo;:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nfrom numpy.linalg import inv\n\n## Play with us!\nbeta = 2     # \"Coefficient of interest\"\n\n## Play with us!\n\n# Let Z have order ell, and X order 1, with Var([X,Z]|u)=VXZ\n\nell = 1 # Play with me too!\n\n# Arbitrary (but deterministic) choice for VXZ\nA = np.sqrt(1/np.arange(1,(ell+1)**2+1)).reshape((ell+1,ell+1)) \n\n## Below here we're less playful.\n\n# Var([X,Z]|u) is constructed so that pos. def.\nVXZ = A.T@A \n\ntruth = (beta,VXZ)"]},{"cell_type":"markdown","metadata":{},"source":["### Monte Carlo Analysis of MLE via GMM\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we take the score to be our moment condition; $Z$ doesn&rsquo;t appear,\nso estimator is just identified.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gmm # Just code defined last class\n\ngmm.gj = mle  # Redefine gj function to use MLE score\n\ndata = dgp(1000,*truth)\n\nsoltn = gmm.two_step_gmm(data)\n\nlimiting_J = iid.chi2(0)\n\nprint(\"b=%f, J=%f, Critical J=%f\" % (soltn.x,soltn.fun,limiting_J.isf(0.05)))"]},{"cell_type":"markdown","metadata":{},"source":["Now our experiment begins.  We set our frequentist hats firmly on our\nheads, and draw repeated samples of data, each generating a\ncorresponding estimate of beta.  Then the empirical distribution of\nthese samples tells us about the *finite* sample performance of our estimator.\n\nWe&rsquo;ll generate a sample of estimates of $b$ by drawing repeated\nsamples of size $N$, until estimates of the covariance of our\nestimates converge:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gmm # Just code defined last class\n\nN = 1000 # Sample size\n\ntol = 1e-3\n\nb_mle = []\nb_nliv = []\nJ_nliv = []\n\nd=0\noldV = 0\nnewV = 1\nwhile d<30 or np.linalg.norm(oldV-newV) > tol:\n    d += 1\n    oldV = newV\n    data = dgp(N,*truth)\n    gmm.gj = mle\n    soltn_mle = gmm.two_step_gmm(data)\n    b_mle.append(soltn_mle.x)\n\n    gmm.gj = nonlinear_iv\n    soltn_nliv = gmm.two_step_gmm(data)\n    b_nliv.append(soltn_nliv.x)\n    J_nliv.append(soltn_nliv.fun)\n\n    newV = np.var(b_nliv)"]},{"cell_type":"markdown","metadata":{},"source":["Now compare MLE & NLIV estimates:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n\n_ = plt.scatter(b_mle,b_nliv)"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}